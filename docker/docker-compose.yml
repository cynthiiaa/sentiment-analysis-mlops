version: "3.8"

services:
  app:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    ports:
      - "7860:7860"
    environment:
      - PYTHONUNBUFFERED=1
      - MLFLOW_TRACKING_URI=http://mlflow:5003
    volumes:
      - model_cache:/app/models
      - ../configs:/app/configs:ro 
    depends_on:
      - mlflow
      - prometheus
    networks:
      - mlops-network

  api:
    build:
      context: ..
      dockerfile: docker/Dockerfile
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - MLFLOW_TRACKING_URI=http://mlflow:5003
    command: python src/api/inference.py
    volumes:
      - model_cache:/app/models
      - ../configs:/app/configs:ro
    depends_on:
      - mlflow
    networks:
      - mlops-network

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    ports:
      - "5003:5003"
    command: mlflow server --host 0.0.0.0 --port 5003
    volumes:
      - mlflow_data:/mlflow
    networks:
      - mlops-network
  
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    networks:
      - mlops-network

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
    depends_on:
      - prometheus
    networks:
      - mlops-network

volumes:
  model_cache:
  mlflow_data:
  prometheus_data:
  grafana_data:

networks:
  mlops-network:
    driver: bridge